{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running GIFT-Eval with tsagentkit\n",
    "\n",
    "Credence-style evaluation notebook adapted for tsagentkit. The structure mirrors\n",
    "`notebooks/Credence.ipynb` so prediction and metric computation are comparable,\n",
    "while the prediction interface is implemented through `TSAgentKitPredictor`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tsagentkit model note\n",
    "\n",
    "`Credence.ipynb` directly calls a hosted API per forecasting window. tsagentkit uses\n",
    "a local GluonTS-compatible predictor (`TSAgentKitPredictor`) instead, so we use a\n",
    "thin adapter class while keeping the same dataset loop and metric pipeline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation\n",
    "\n",
    "This notebook assumes Python 3.11 with `tsagentkit` and benchmark dependencies installed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional when running outside the project environment:\n",
    "# %pip install \"tsagentkit[tsfm,gift-eval]\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the required dependencies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gluonts.ev.metrics import (\n",
    "    MAE,\n",
    "    MAPE,\n",
    "    MASE,\n",
    "    MSE,\n",
    "    MSIS,\n",
    "    ND,\n",
    "    NRMSE,\n",
    "    RMSE,\n",
    "    SMAPE,\n",
    "    MeanWeightedSumQuantileLoss,\n",
    ")\n",
    "from gluonts.model import evaluate_model\n",
    "\n",
    "from tsagentkit.gift_eval.data import Dataset, MED_LONG_DATASETS, SHORT_DATASETS\n",
    "from tsagentkit.gift_eval.dataset_properties import DATASET_PROPERTIES\n",
    "from tsagentkit.gift_eval.eval import GIFTEval\n",
    "from tsagentkit.gift_eval.predictor import QUANTILES, TSAgentKitPredictor\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the configuration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path configurations\n",
    "storage_path = Path(\"./data/gift-eval\")\n",
    "out_dir = Path(\"./results/tsagentkit\")\n",
    "\n",
    "# Model configuration\n",
    "model_name = \"tsagentkit\"\n",
    "mode = \"standard\"\n",
    "preload_adapters = [\"chronos\"]\n",
    "batch_size = 512\n",
    "\n",
    "# Auxiliary configurations\n",
    "seed = 0\n",
    "download_data = False\n",
    "\n",
    "if download_data:\n",
    "    GIFTEval.download_data(storage_path=storage_path)\n",
    "\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"storage_path={storage_path.resolve()}\")\n",
    "print(f\"out_dir={out_dir.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment configurations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_datasets = \" \".join(SHORT_DATASETS)\n",
    "med_long_datasets = \" \".join(MED_LONG_DATASETS)\n",
    "dataset_properties = DATASET_PROPERTIES\n",
    "\n",
    "short_datasets.split()[:5], len(short_datasets.split()), len(med_long_datasets.split())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auxiliary functions\n",
    "def set_seed(seed: int) -> None:\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "\n",
    "class TSAgentKitGiftAdapter:\n",
    "    \"\"\"Adapter to keep evaluate_model(...) usage aligned with Credence notebook style.\"\"\"\n",
    "\n",
    "    def __init__(self, predictor: TSAgentKitPredictor):\n",
    "        self.predictor = predictor\n",
    "\n",
    "    def predict(self, test_data, **kwargs):\n",
    "        return self.predictor.predict(test_data, **kwargs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment wrapper.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_row = [\n",
    "    \"dataset\",\n",
    "    \"model\",\n",
    "    \"eval_metrics/MSE[mean]\",\n",
    "    \"eval_metrics/MSE[0.5]\",\n",
    "    \"eval_metrics/MAE[mean]\",\n",
    "    \"eval_metrics/MAE[0.5]\",\n",
    "    \"eval_metrics/MASE[0.5]\",\n",
    "    \"eval_metrics/MAPE[0.5]\",\n",
    "    \"eval_metrics/sMAPE[0.5]\",\n",
    "    \"eval_metrics/MSIS\",\n",
    "    \"eval_metrics/RMSE[mean]\",\n",
    "    \"eval_metrics/NRMSE[mean]\",\n",
    "    \"eval_metrics/ND[0.5]\",\n",
    "    \"eval_metrics/mean_weighted_sum_quantile_loss\",\n",
    "    \"domain\",\n",
    "    \"num_variates\",\n",
    "]\n",
    "\n",
    "\n",
    "def run_gift_eval(zs: bool = False, save: bool = False, verbose: bool = True):\n",
    "    set_seed(seed)\n",
    "\n",
    "    # Match official notebook behavior: evaluate the union of short + med/long datasets.\n",
    "    all_datasets = sorted(set(short_datasets.split() + med_long_datasets.split()))\n",
    "    dataset_properties_map = dataset_properties\n",
    "\n",
    "    metrics = [\n",
    "        MSE(forecast_type=\"mean\"),\n",
    "        MSE(forecast_type=0.5),\n",
    "        MAE(forecast_type=\"mean\"),\n",
    "        MAE(forecast_type=0.5),\n",
    "        MASE(),\n",
    "        MAPE(),\n",
    "        SMAPE(),\n",
    "        MSIS(),\n",
    "        RMSE(),\n",
    "        NRMSE(),\n",
    "        ND(),\n",
    "        MeanWeightedSumQuantileLoss(quantile_levels=QUANTILES),\n",
    "    ]\n",
    "\n",
    "    csv_file_path = out_dir / \"all_results.csv\"\n",
    "\n",
    "    pretty_names = {\n",
    "        \"saugeenday\": \"saugeen\",\n",
    "        \"temperature_rain_with_missing\": \"temperature_rain\",\n",
    "        \"kdd_cup_2018_with_missing\": \"kdd_cup_2018\",\n",
    "        \"car_parts_with_missing\": \"car_parts\",\n",
    "    }\n",
    "\n",
    "    if save and not csv_file_path.exists():\n",
    "        with csv_file_path.open(\"a\", newline=\"\") as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerow(base_row)\n",
    "\n",
    "    if save and csv_file_path.exists():\n",
    "        df_res_done = pd.read_csv(csv_file_path)\n",
    "        done_datasets = set(df_res_done[\"dataset\"].tolist())\n",
    "    else:\n",
    "        df_res_done = pd.DataFrame(columns=base_row)\n",
    "        done_datasets = set()\n",
    "\n",
    "    df_res = pd.DataFrame(columns=base_row)\n",
    "\n",
    "    # Optional zero-shot subset used in official notebooks for certain pretraining comparisons.\n",
    "    if zs:\n",
    "        excluded = {\n",
    "            \"solar/H\",\n",
    "            \"m4_monthly\",\n",
    "            \"m4_weekly\",\n",
    "            \"m4_daily\",\n",
    "            \"m4_hourly\",\n",
    "            \"electricity/15T\",\n",
    "            \"electricity/H\",\n",
    "            \"electricity/W\",\n",
    "            \"kdd_cup_2018_with_missing/D\",\n",
    "            \"kdd_cup_2018_with_missing/H\",\n",
    "            \"temperature_rain_with_missing\",\n",
    "        }\n",
    "    else:\n",
    "        excluded = set()\n",
    "\n",
    "    predictor = TSAgentKitPredictor(\n",
    "        mode=mode,\n",
    "        batch_size=batch_size,\n",
    "        preload_adapters=preload_adapters,\n",
    "    )\n",
    "    model = TSAgentKitGiftAdapter(predictor)\n",
    "\n",
    "    import inspect\n",
    "    evaluate_model_params = set(inspect.signature(evaluate_model).parameters)\n",
    "    supports_eval_batch_size = \"batch_size\" in evaluate_model_params\n",
    "\n",
    "    try:\n",
    "        for ds_name in all_datasets:\n",
    "            if ds_name in excluded:\n",
    "                continue\n",
    "\n",
    "            set_seed(seed)\n",
    "            for term in (\"short\", \"medium\", \"long\"):\n",
    "                if term in {\"medium\", \"long\"} and ds_name not in med_long_datasets.split():\n",
    "                    continue\n",
    "\n",
    "                if \"/\" in ds_name:\n",
    "                    ds_key, ds_freq = ds_name.split(\"/\", maxsplit=1)\n",
    "                    ds_key = pretty_names.get(ds_key.lower(), ds_key.lower())\n",
    "                else:\n",
    "                    ds_key = pretty_names.get(ds_name.lower(), ds_name.lower())\n",
    "                    ds_freq = str(dataset_properties_map[ds_key][\"frequency\"])\n",
    "\n",
    "                ds_config = f\"{ds_key}/{ds_freq}/{term}\"\n",
    "\n",
    "                if ds_config in done_datasets:\n",
    "                    df_res = pd.concat(\n",
    "                        [df_res, df_res_done.loc[df_res_done[\"dataset\"] == ds_config]],\n",
    "                        ignore_index=True,\n",
    "                    )\n",
    "                    continue\n",
    "\n",
    "                temp_dataset = Dataset(\n",
    "                    name=ds_name,\n",
    "                    term=term,\n",
    "                    to_univariate=False,\n",
    "                    storage_path=storage_path,\n",
    "                )\n",
    "                to_univariate = temp_dataset.target_dim != 1\n",
    "                dataset = Dataset(\n",
    "                    name=ds_name,\n",
    "                    term=term,\n",
    "                    to_univariate=to_univariate,\n",
    "                    storage_path=storage_path,\n",
    "                )\n",
    "\n",
    "                predictor.h = dataset.prediction_length\n",
    "                predictor.freq = dataset.freq\n",
    "\n",
    "                if verbose:\n",
    "                    print(\n",
    "                        f\"Dataset: {ds_name}, term={term}, \"\n",
    "                        f\"freq={dataset.freq}, H={dataset.prediction_length}\"\n",
    "                    )\n",
    "\n",
    "                eval_kwargs = dict(\n",
    "                    test_data=dataset.test_data,\n",
    "                    metrics=metrics,\n",
    "                    axis=None,\n",
    "                    mask_invalid_label=True,\n",
    "                    allow_nan_forecast=False,\n",
    "                )\n",
    "                if supports_eval_batch_size:\n",
    "                    eval_kwargs[\"batch_size\"] = batch_size\n",
    "\n",
    "                res = evaluate_model(model, **eval_kwargs)\n",
    "\n",
    "                row = [\n",
    "                    ds_config,\n",
    "                    model_name,\n",
    "                    float(res[\"MSE[mean]\"].iloc[0]),\n",
    "                    float(res[\"MSE[0.5]\"].iloc[0]),\n",
    "                    float(res[\"MAE[mean]\"].iloc[0]),\n",
    "                    float(res[\"MAE[0.5]\"].iloc[0]),\n",
    "                    float(res[\"MASE[0.5]\"].iloc[0]),\n",
    "                    float(res[\"MAPE[0.5]\"].iloc[0]),\n",
    "                    float(res[\"sMAPE[0.5]\"].iloc[0]),\n",
    "                    float(res[\"MSIS\"].iloc[0]),\n",
    "                    float(res[\"RMSE[mean]\"].iloc[0]),\n",
    "                    float(res[\"NRMSE[mean]\"].iloc[0]),\n",
    "                    float(res[\"ND[0.5]\"].iloc[0]),\n",
    "                    float(res[\"mean_weighted_sum_quantile_loss\"].iloc[0]),\n",
    "                    dataset_properties_map[ds_key][\"domain\"],\n",
    "                    dataset_properties_map[ds_key][\"num_variates\"],\n",
    "                ]\n",
    "\n",
    "                if save:\n",
    "                    with csv_file_path.open(\"a\", newline=\"\") as csvfile:\n",
    "                        writer = csv.writer(csvfile)\n",
    "                        writer.writerow(row)\n",
    "                    if verbose:\n",
    "                        print(f\"Results for {ds_config} written to {csv_file_path}\")\n",
    "\n",
    "                df_res.loc[len(df_res)] = row\n",
    "\n",
    "                if verbose:\n",
    "                    print(\n",
    "                        f\"MASE={row[6]:.6f} | CRPS={row[13]:.6f}\"\n",
    "                    )\n",
    "    finally:\n",
    "        predictor.close()\n",
    "\n",
    "    baseline_path = Path(\"./results/seasonal_naive/all_results.csv\")\n",
    "    if baseline_path.exists() and len(df_res) > 0:\n",
    "        seasonal_naive = pd.read_csv(baseline_path).sort_values(\"dataset\")\n",
    "        df = df_res.sort_values(\"dataset\").copy()\n",
    "\n",
    "        baseline_by_dataset = seasonal_naive.set_index(\"dataset\")\n",
    "        aligned = baseline_by_dataset.loc[df[\"dataset\"]]\n",
    "        df[\"normalized MASE\"] = (\n",
    "            df[\"eval_metrics/MASE[0.5]\"].to_numpy() /\n",
    "            aligned[\"eval_metrics/MASE[0.5]\"].to_numpy()\n",
    "        )\n",
    "        df[\"normalized CRPS\"] = (\n",
    "            df[\"eval_metrics/mean_weighted_sum_quantile_loss\"].to_numpy() /\n",
    "            aligned[\"eval_metrics/mean_weighted_sum_quantile_loss\"].to_numpy()\n",
    "        )\n",
    "\n",
    "        mase = float(np.exp(np.mean(np.log(df[\"normalized MASE\"].to_numpy()))))\n",
    "        crps = float(np.exp(np.mean(np.log(df[\"normalized CRPS\"].to_numpy()))))\n",
    "    else:\n",
    "        mase = float(\"nan\")\n",
    "        crps = float(\"nan\")\n",
    "\n",
    "    return mase, crps, df_res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start the experiment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mase, crps, df_res = run_gift_eval(verbose=True, save=True)\n",
    "\n",
    "if np.isfinite(mase) and np.isfinite(crps):\n",
    "    print(f\"Final GIFT-Eval performance of {model_name}:\\nMASE = {mase}, CRPS = {crps}\")\n",
    "else:\n",
    "    print(\n",
    "        \"Evaluation finished. Normalized MASE/CRPS not computed \"\n",
    "        \"(missing ./results/seasonal_naive/all_results.csv).\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_csv = out_dir / \"all_results.csv\"\n",
    "eval_df = pd.read_csv(results_csv)\n",
    "print(f\"Rows: {len(eval_df)}\")\n",
    "eval_df.tail(10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
