{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tsagentkit (GIFT-Eval)\n",
    "\n",
    "Official-style single notebook for running tsagentkit on GIFT-Eval and exporting `all_results.csv`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements and Installation\n",
    "\n",
    "This notebook assumes a Python 3.11 environment with tsagentkit and GIFT-Eval dependencies installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional when not already in a local dev environment:\n",
    "# %pip install \"tsagentkit[tsfm,gift-eval]\" pandas numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from tsagentkit.gift_eval.data import DATASETS_WITH_TERMS, FULL_MATRIX_SIZE\n",
    "from tsagentkit.gift_eval.eval import GIFTEval, RESULT_COLUMNS\n",
    "from tsagentkit.gift_eval.predictor import TSAgentKitPredictor\n",
    "from tsagentkit.gift_eval.score import compute_aggregate_scores, compute_normalized_scores\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "logger = logging.getLogger(\"tsagentkit_gifteval_notebook\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runtime configuration\n",
    "storage_path = Path(\"./data/gift-eval\")\n",
    "output_path = Path(\"./results/tsagentkit\")\n",
    "\n",
    "model_name = \"tsagentkit\"\n",
    "mode = \"standard\"\n",
    "batch_size = 512\n",
    "preload_adapters = [\"chronos\"]\n",
    "overwrite_results = True\n",
    "\n",
    "# Submission metadata (for config.json in the benchmark repo)\n",
    "submission_meta = {\n",
    "    \"model\": model_name,\n",
    "    \"model_type\": \"agentic\",\n",
    "    \"model_dtype\": \"float32\",\n",
    "    \"model_link\": \"https://github.com/your-org/tsagentkit\",\n",
    "    \"code_link\": \"https://github.com/your-org/tsagentkit/blob/main/benchmarks/gift_eval/notebooks/tsagentkit.ipynb\",\n",
    "    \"org\": \"YourOrg\",\n",
    "    \"testdata_leakage\": \"No\",\n",
    "    \"replication_code_available\": \"Yes\",\n",
    "}\n",
    "\n",
    "print(f\"Storage path: {storage_path.resolve()}\")\n",
    "print(f\"Output path: {output_path.resolve()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_data = False\n",
    "if download_data:\n",
    "    GIFTEval.download_data(storage_path=storage_path)\n",
    "\n",
    "output_path.mkdir(parents=True, exist_ok=True)\n",
    "print(\"Ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_predictor(batch_size: int = 512) -> TSAgentKitPredictor:\n",
    "    return TSAgentKitPredictor(\n",
    "        mode=mode,\n",
    "        batch_size=batch_size,\n",
    "        preload_adapters=preload_adapters,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the evaluator\n",
    "\n",
    "Each benchmark row is defined by a `(dataset_name, term)` combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_tsagentkit(\n",
    "    predictor: TSAgentKitPredictor,\n",
    "    dataset_name: str,\n",
    "    term: str,\n",
    "    output_path: Path,\n",
    "    storage_path: Path,\n",
    "    mode: str,\n",
    "    preload_adapters: list[str],\n",
    "    batch_size: int,\n",
    "    model_name: str,\n",
    ") -> pd.DataFrame:\n",
    "    evaluator = GIFTEval(\n",
    "        dataset_name=dataset_name,\n",
    "        term=term,\n",
    "        output_path=output_path,\n",
    "        storage_path=storage_path,\n",
    "        mode=mode,\n",
    "        preload_adapters=preload_adapters,\n",
    "    )\n",
    "    return evaluator.evaluate_predictor(\n",
    "        predictor=predictor,\n",
    "        batch_size=batch_size,\n",
    "        overwrite=False,\n",
    "        model_name=model_name,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoke_combinations = [\n",
    "    (\"m4_weekly\", \"short\"),\n",
    "    (\"bizitobs_l2c/H\", \"short\"),\n",
    "    (\"bizitobs_l2c/H\", \"medium\"),\n",
    "    (\"bizitobs_l2c/H\", \"long\"),\n",
    "]\n",
    "\n",
    "use_full_matrix = False\n",
    "combinations = list(DATASETS_WITH_TERMS) if use_full_matrix else smoke_combinations\n",
    "\n",
    "results_csv = output_path / \"all_results.csv\"\n",
    "if overwrite_results and results_csv.exists():\n",
    "    results_csv.unlink()\n",
    "\n",
    "if not storage_path.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Dataset path not found: {storage_path}. Set download_data=True first.\"\n",
    "    )\n",
    "\n",
    "predictor = build_predictor(batch_size=batch_size)\n",
    "try:\n",
    "    for idx, (dataset_name, term) in enumerate(combinations, start=1):\n",
    "        logger.info(\"[%d/%d] %s/%s\", idx, len(combinations), dataset_name, term)\n",
    "        df = evaluate_tsagentkit(\n",
    "            predictor=predictor,\n",
    "            dataset_name=dataset_name,\n",
    "            term=term,\n",
    "            output_path=output_path,\n",
    "            storage_path=storage_path,\n",
    "            mode=mode,\n",
    "            preload_adapters=preload_adapters,\n",
    "            batch_size=batch_size,\n",
    "            model_name=model_name,\n",
    "        )\n",
    "        row = df.iloc[-1]\n",
    "        logger.info(\n",
    "            \"MASE=%.4f | sMAPE=%.4f | CRPS=%.4f\",\n",
    "            float(row[\"eval_metrics/MASE[0.5]\"]),\n",
    "            float(row[\"eval_metrics/sMAPE[0.5]\"]),\n",
    "            float(row[\"eval_metrics/mean_weighted_sum_quantile_loss\"]),\n",
    "        )\n",
    "finally:\n",
    "    predictor.close()\n",
    "\n",
    "print(f\"Saved: {results_csv}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can access the complete combination of datasets with the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASETS_WITH_TERMS[:3], len(DATASETS_WITH_TERMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = pd.read_csv(results_csv)\n",
    "print(f\"Rows: {len(eval_df)}\")\n",
    "eval_df.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission Readiness Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert list(eval_df.columns) == RESULT_COLUMNS, \"Unexpected result columns.\"\n",
    "assert not eval_df[\"dataset\"].duplicated().any(), \"Duplicate dataset rows detected.\"\n",
    "\n",
    "if use_full_matrix:\n",
    "    assert len(eval_df) == FULL_MATRIX_SIZE, (\n",
    "        f\"Expected {FULL_MATRIX_SIZE} rows for full matrix; got {len(eval_df)}\"\n",
    "    )\n",
    "\n",
    "print(\"Result schema checks passed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproducibility statement\n",
    "\n",
    "This notebook uses tsagentkit's packaged GIFT-Eval integration and writes standard\n",
    "`all_results.csv` rows. It also supports optional normalized score computation against\n",
    "a seasonal-naive baseline file when available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = {\n",
    "    \"aggregate\": compute_aggregate_scores(eval_df),\n",
    "}\n",
    "\n",
    "baseline_file = Path(\"./results/seasonal_naive/all_results.csv\")\n",
    "if baseline_file.exists():\n",
    "    baseline_df = pd.read_csv(baseline_file)\n",
    "    summary[\"normalized\"] = compute_normalized_scores(eval_df, baseline_df)\n",
    "\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Changelog\n\n- 2026-02-11: Remove backtest mode parameter (multi-model is now default)\n- 2026-02-11: Second-pass polish for official-style readability, submission metadata, and schema checks."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}