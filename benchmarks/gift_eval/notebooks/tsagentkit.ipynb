{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Running GIFT-Eval with tsagentkit (DEPRECATED - v1.x only)\n\n**⚠️ This notebook uses tsagentkit v1.x API which has been removed in v2.0.**\n\nFor v2.0 ensemble forecasting, use the updated `tsagentkit_quick.py` script instead:\n```bash\npython tsagentkit_quick.py --help\n```\n\nThis notebook is kept for reference but requires tsagentkit v1.x to run."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation\n",
    "\n",
    "This notebook assumes Python 3.11 with `tsagentkit` and benchmark dependencies installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional when running outside the project environment:\n",
    "# %pip install \"tsagentkit[tsfm,gift-eval]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dependencies and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import warnings\n",
    "import csv\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "logging.getLogger(\"gluonts\").setLevel(logging.ERROR)\n",
    "\n",
    "import pandas as pd\n",
    "from gluonts.ev.metrics import (\n",
    "    MAE,\n",
    "    MAPE,\n",
    "    MASE,\n",
    "    MSE,\n",
    "    MSIS,\n",
    "    ND,\n",
    "    NRMSE,\n",
    "    RMSE,\n",
    "    SMAPE,\n",
    "    MeanWeightedSumQuantileLoss,\n",
    ")\n",
    "from gluonts.model import evaluate_model\n",
    "from gluonts.time_feature import get_seasonality\n",
    "\n",
    "from tsagentkit.gift_eval.data import Dataset\n",
    "from tsagentkit.gift_eval.predictor import QUANTILES, TSAgentKitPredictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Paths\nstorage_path = Path(\"./data/gift-eval\")\noutput_dir = Path(\"./results/tsagentkit\")\nos.makedirs(output_dir, exist_ok=True)\n\n# Model configuration\nmodel_name = \"tsagentkit\"\nmode = \"standard\"\npreload_adapters = [\"chronos\"]\nbatch_size = 512\n\n# Debug mode: use only a few short datasets for quick testing\ndebug = False\n\n# Data download: Set to True to download GIFT-eval datasets if not already available\n# Data source: https://github.com/SalesforceAIResearch/gift-eval\n# Expected data structure: {storage_path}/{dataset_name}/{term}/\ndownload_data = False\n\n# Datasets (short + med/long)\nshort_datasets = [\n    \"m4_yearly\", \"m4_quarterly\", \"m4_monthly\", \"m4_weekly\", \"m4_daily\", \"m4_hourly\",\n    \"electricity/15T\", \"electricity/H\", \"electricity/D\", \"electricity/W\",\n    \"solar/10T\", \"solar/H\", \"solar/D\", \"solar/W\",\n    \"hospital\", \"covid_deaths\",\n    \"us_births/D\", \"us_births/M\", \"us_births/W\",\n    \"saugeenday/D\", \"saugeenday/M\", \"saugeenday/W\",\n    \"temperature_rain_with_missing\",\n    \"kdd_cup_2018_with_missing/H\", \"kdd_cup_2018_with_missing/D\",\n    \"car_parts_with_missing\",\n    \"restaurant\",\n    \"hierarchical_sales/D\", \"hierarchical_sales/W\",\n    \"LOOP_SEATTLE/5T\", \"LOOP_SEATTLE/H\", \"LOOP_SEATTLE/D\",\n    \"SZ_TAXI/15T\", \"SZ_TAXI/H\",\n    \"M_DENSE/H\", \"M_DENSE/D\",\n    \"ett1/15T\", \"ett1/H\", \"ett1/D\", \"ett1/W\",\n    \"ett2/15T\", \"ett2/H\", \"ett2/D\", \"ett2/W\",\n    \"jena_weather/10T\", \"jena_weather/H\", \"jena_weather/D\",\n    \"bitbrains_fast_storage/5T\", \"bitbrains_fast_storage/H\",\n    \"bitbrains_rnd/5T\", \"bitbrains_rnd/H\",\n    \"bizitobs_application\", \"bizitobs_service\",\n    \"bizitobs_l2c/5T\", \"bizitobs_l2c/H\"\n]\n\nmed_long_datasets = [\n    \"electricity/15T\", \"electricity/H\",\n    \"solar/10T\", \"solar/H\",\n    \"kdd_cup_2018_with_missing/H\",\n    \"LOOP_SEATTLE/5T\", \"LOOP_SEATTLE/H\",\n    \"SZ_TAXI/15T\",\n    \"M_DENSE/H\",\n    \"ett1/15T\", \"ett1/H\",\n    \"ett2/15T\", \"ett2/H\",\n    \"jena_weather/10T\", \"jena_weather/H\",\n    \"bitbrains_fast_storage/5T\",\n    \"bitbrains_rnd/5T\",\n    \"bizitobs_application\", \"bizitobs_service\",\n    \"bizitobs_l2c/5T\", \"bizitobs_l2c/H\"\n]\n\n# Dataset properties\ndataset_properties_map = {\n    \"m4_yearly\": {\"frequency\": \"Y\", \"domain\": \"finance\", \"num_variates\": 1},\n    \"m4_quarterly\": {\"frequency\": \"Q\", \"domain\": \"finance\", \"num_variates\": 1},\n    \"m4_monthly\": {\"frequency\": \"M\", \"domain\": \"finance\", \"num_variates\": 1},\n    \"m4_weekly\": {\"frequency\": \"W\", \"domain\": \"finance\", \"num_variates\": 1},\n    \"m4_daily\": {\"frequency\": \"D\", \"domain\": \"finance\", \"num_variates\": 1},\n    \"m4_hourly\": {\"frequency\": \"H\", \"domain\": \"finance\", \"num_variates\": 1},\n    \"electricity\": {\"frequency\": \"H\", \"domain\": \"energy\", \"num_variates\": 370},\n    \"solar\": {\"frequency\": \"10T\", \"domain\": \"energy\", \"num_variates\": 137},\n    \"hospital\": {\"frequency\": \"H\", \"domain\": \"health\", \"num_variates\": 1},\n    \"covid_deaths\": {\"frequency\": \"W\", \"domain\": \"health\", \"num_variates\": 1},\n    \"us_births\": {\"frequency\": \"D\", \"domain\": \"demographics\", \"num_variates\": 1},\n    \"saugeen\": {\"frequency\": \"D\", \"domain\": \"environment\", \"num_variates\": 1},\n    \"temperature_rain\": {\"frequency\": \"D\", \"domain\": \"environment\", \"num_variates\": 1},\n    \"kdd_cup_2018\": {\"frequency\": \"H\", \"domain\": \"traffic\", \"num_variates\": 1},\n    \"car_parts\": {\"frequency\": \"M\", \"domain\": \"business\", \"num_variates\": 1},\n    \"restaurant\": {\"frequency\": \"W\", \"domain\": \"business\", \"num_variates\": 1},\n    \"hierarchical_sales\": {\"frequency\": \"W\", \"domain\": \"business\", \"num_variates\": 1},\n    \"loop_seattle\": {\"frequency\": \"5T\", \"domain\": \"traffic\", \"num_variates\": 323},\n    \"sz_taxi\": {\"frequency\": \"15T\", \"domain\": \"traffic\", \"num_variates\": 10215},\n    \"m_dense\": {\"frequency\": \"H\", \"domain\": \"traffic\", \"num_variates\": 963},\n    \"ett1\": {\"frequency\": \"H\", \"domain\": \"energy\", \"num_variates\": 7},\n    \"ett2\": {\"frequency\": \"H\", \"domain\": \"energy\", \"num_variates\": 7},\n    \"jena_weather\": {\"frequency\": \"10T\", \"domain\": \"environment\", \"num_variates\": 21},\n    \"bitbrains_fast_storage\": {\"frequency\": \"5T\", \"domain\": \"it\", \"num_variates\": 1},\n    \"bitbrains_rnd\": {\"frequency\": \"5T\", \"domain\": \"it\", \"num_variates\": 1},\n    \"bizitobs_application\": {\"frequency\": \"H\", \"domain\": \"it\", \"num_variates\": 1},\n    \"bizitobs_service\": {\"frequency\": \"H\", \"domain\": \"it\", \"num_variates\": 1},\n    \"bizitobs_l2c\": {\"frequency\": \"5T\", \"domain\": \"it\", \"num_variates\": 1}\n}\n\n# Combined datasets\nall_datasets = sorted(set(short_datasets + med_long_datasets))\n\n# Debug mode: use only a few datasets\nif debug:\n    all_datasets = all_datasets[:5]\n\n# Pretty names for dataset keys\npretty_names = {\n    \"saugeenday\": \"saugeen\",\n    \"temperature_rain_with_missing\": \"temperature_rain\",\n    \"kdd_cup_2018_with_missing\": \"kdd_cup_2018\",\n    \"car_parts_with_missing\": \"car_parts\",\n}\n\n# Download data if requested\nif download_data:\n    from tsagentkit.gift_eval.eval import GIFTEval\n    GIFTEval.download_data(storage_path=storage_path)\n\nprint(f\"storage_path={storage_path.resolve()}\")\nprint(f\"output_dir={output_dir.resolve()}\")\nprint(f\"Debug mode: {debug}\")\nprint(f\"Download data: {download_data}\")\nprint(f\"Total datasets: {len(all_datasets)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\n",
    "    MSE(forecast_type=\"mean\"),\n",
    "    MSE(forecast_type=0.5),\n",
    "    MAE(),\n",
    "    MASE(),\n",
    "    MAPE(),\n",
    "    SMAPE(),\n",
    "    MSIS(),\n",
    "    RMSE(),\n",
    "    NRMSE(),\n",
    "    ND(),\n",
    "    MeanWeightedSumQuantileLoss(\n",
    "        quantile_levels=QUANTILES\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_path = output_dir / \"all_results.csv\"\n",
    "header = [\n",
    "    \"dataset\",\n",
    "    \"model\",\n",
    "    \"eval_metrics/MSE[mean]\",\n",
    "    \"eval_metrics/MSE[0.5]\",\n",
    "    \"eval_metrics/MAE[0.5]\",\n",
    "    \"eval_metrics/MASE[0.5]\",\n",
    "    \"eval_metrics/MAPE[0.5]\",\n",
    "    \"eval_metrics/sMAPE[0.5]\",\n",
    "    \"eval_metrics/MSIS\",\n",
    "    \"eval_metrics/RMSE[mean]\",\n",
    "    \"eval_metrics/NRMSE[mean]\",\n",
    "    \"eval_metrics/ND[0.5]\",\n",
    "    \"eval_metrics/mean_weighted_sum_quantile_loss\",\n",
    "    \"domain\",\n",
    "    \"num_variates\",\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "# Check if evaluate_model supports batch_size parameter\n",
    "import inspect\n",
    "evaluate_model_params = set(inspect.signature(evaluate_model).parameters)\n",
    "supports_eval_batch_size = \"batch_size\" in evaluate_model_params\n",
    "\n",
    "# Create predictor\n",
    "predictor = TSAgentKitPredictor(\n",
    "    mode=mode,\n",
    "    batch_size=batch_size,\n",
    "    preload_adapters=preload_adapters,\n",
    ")\n",
    "\n",
    "try:\n",
    "    for ds_name in all_datasets:\n",
    "        print(f\"Processing dataset: {ds_name}\")\n",
    "        terms = [\"short\", \"medium\", \"long\"]\n",
    "        \n",
    "        for term in terms:\n",
    "            if (term == \"medium\" or term == \"long\") and ds_name not in med_long_datasets:\n",
    "                continue\n",
    "\n",
    "            # Parse dataset key and frequency\n",
    "            if \"/\" in ds_name:\n",
    "                ds_key = ds_name.split(\"/\")[0]\n",
    "                ds_freq = ds_name.split(\"/\")[1]\n",
    "                ds_key = ds_key.lower()\n",
    "                ds_key = pretty_names.get(ds_key, ds_key)\n",
    "            else:\n",
    "                ds_key = ds_name.lower()\n",
    "                ds_key = pretty_names.get(ds_key, ds_key)\n",
    "                ds_freq = dataset_properties_map[ds_key][\"frequency\"]\n",
    "\n",
    "            ds_config = f\"{ds_key}/{ds_freq}/{term}\"\n",
    "\n",
    "            # Create dataset\n",
    "            to_univariate = False if Dataset(\n",
    "                name=ds_name, term=term, to_univariate=False, storage_path=storage_path\n",
    "            ).target_dim == 1 else True\n",
    "\n",
    "            dataset = Dataset(\n",
    "                name=ds_name, term=term, to_univariate=to_univariate, storage_path=storage_path\n",
    "            )\n",
    "\n",
    "            # Update predictor parameters for this dataset\n",
    "            predictor.h = dataset.prediction_length\n",
    "            predictor.freq = dataset.freq\n",
    "\n",
    "            # Adjust batch size if needed for memory\n",
    "            current_batch_size = batch_size\n",
    "            while current_batch_size >= 1:\n",
    "                try:\n",
    "                    predictor.batch_size = current_batch_size\n",
    "                    season_length = get_seasonality(dataset.freq)\n",
    "                    \n",
    "                    eval_kwargs = dict(\n",
    "                        test_data=dataset.test_data,\n",
    "                        metrics=metrics,\n",
    "                        axis=None,\n",
    "                        mask_invalid_label=True,\n",
    "                        allow_nan_forecast=False,\n",
    "                        seasonality=season_length,\n",
    "                    )\n",
    "                    if supports_eval_batch_size:\n",
    "                        eval_kwargs[\"batch_size\"] = current_batch_size\n",
    "                    \n",
    "                    res = evaluate_model(predictor, **eval_kwargs)\n",
    "                    break  # Success\n",
    "                except RuntimeError as e:\n",
    "                    if (\"CUDA out of memory\" in str(e) or \"out of memory\" in str(e).lower()):\n",
    "                        print(f\"CUDA out of memory with batch_size={current_batch_size}, halving...\")\n",
    "                        current_batch_size //= 2\n",
    "                        if current_batch_size < 1:\n",
    "                            raise RuntimeError(\"Batch size reduced below 1; cannot proceed.\") from e\n",
    "                    else:\n",
    "                        raise\n",
    "\n",
    "            result_row = [\n",
    "                ds_config,\n",
    "                model_name,\n",
    "                res[\"MSE[mean]\"][0],\n",
    "                res[\"MSE[0.5]\"][0],\n",
    "                res[\"MAE[0.5]\"][0],\n",
    "                res[\"MASE[0.5]\"][0],\n",
    "                res[\"MAPE[0.5]\"][0],\n",
    "                res[\"sMAPE[0.5]\"][0],\n",
    "                res[\"MSIS\"][0],\n",
    "                res[\"RMSE[mean]\"][0],\n",
    "                res[\"NRMSE[mean]\"][0],\n",
    "                res[\"ND[0.5]\"][0],\n",
    "                res[\"mean_weighted_sum_quantile_loss\"][0],\n",
    "                dataset_properties_map[ds_key][\"domain\"],\n",
    "                dataset_properties_map[ds_key][\"num_variates\"],\n",
    "            ]\n",
    "\n",
    "            results.append((ds_config, result_row))\n",
    "            print(f\"  {ds_config}: MASE={result_row[5]:.6f}, CRPS={result_row[12]:.6f}\")\n",
    "\n",
    "finally:\n",
    "    predictor.close()\n",
    "\n",
    "# Write results to CSV\n",
    "with open(csv_file_path, \"w\", newline=\"\") as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(header)\n",
    "    for _, row in results:\n",
    "        writer.writerow(row)\n",
    "\n",
    "print(f\"\\nResults written to {csv_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute normalized scores vs Seasonal Naive baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_path = Path(\"./results/seasonal_naive/all_results.csv\")\n",
    "\n",
    "if baseline_path.exists():\n",
    "    seasonal_naive = pd.read_csv(baseline_path).sort_values(\"dataset\")\n",
    "    df = pd.DataFrame([row for _, row in results], columns=header).sort_values(\"dataset\")\n",
    "\n",
    "    baseline_by_dataset = seasonal_naive.set_index(\"dataset\")\n",
    "    aligned = baseline_by_dataset.loc[df[\"dataset\"]]\n",
    "\n",
    "    df[\"normalized MASE\"] = (\n",
    "        df[\"eval_metrics/MASE[0.5]\"].to_numpy() /\n",
    "        aligned[\"eval_metrics/MASE[0.5]\"].to_numpy()\n",
    "    )\n",
    "    df[\"normalized CRPS\"] = (\n",
    "        df[\"eval_metrics/mean_weighted_sum_quantile_loss\"].to_numpy() /\n",
    "        aligned[\"eval_metrics/mean_weighted_sum_quantile_loss\"].to_numpy()\n",
    "    )\n",
    "\n",
    "    mase = float(np.exp(np.mean(np.log(df[\"normalized MASE\"].to_numpy())))\n",
    "    crps = float(np.exp(np.mean(np.log(df[\"normalized CRPS\"].to_numpy())))\n",
    "\n",
    "    print(f\"Final GIFT-Eval performance of {model_name}:\")\n",
    "    print(f\"MASE = {mase}\")\n",
    "    print(f\"CRPS = {crps}\")\n",
    "else:\n",
    "    print(\n",
    "        \"Normalized MASE/CRPS not computed \"\n",
    "        \"(missing ./results/seasonal_naive/all_results.csv).\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = pd.read_csv(csv_file_path)\n",
    "print(f\"Rows: {len(eval_df)}\")\n",
    "eval_df.tail(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}